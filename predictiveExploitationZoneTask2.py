from sre_constants import RANGE
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from pyspark import SparkConf
from pyspark.context import SparkContext
from numpy import array
from math import sqrt
from pyspark.mllib.tree import RandomForest, RandomForestModel
from pyspark.mllib.util import MLUtils
from pyspark.mllib.regression import LabeledPoint

if __name__ == "__main__":
    spark = (
        SparkSession.builder.master(f"local[*]")
        .appName("myApp")
        .config(
            "spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1"
        )
        .getOrCreate()
    )

    # Pull the data from mongo into RDDs
    GM_DF__sentenceSentimentScore = (
        spark.read.format("mongo")
        .option(
            "uri", f"mongodb://127.0.0.1/bdm_project2022.GM_DF__sentenceSentimentScore"
        )
        .load()
    )
sc = SparkContext.getOrCreate()

GM_RDD__sentenceSentimentScore = GM_DF__sentenceSentimentScore.rdd
GM_RDD__labeledPoint = GM_RDD__sentenceSentimentScore.map(
    lambda x: (
        float(x[2]),
        (
            (float(x[1])),
            (float(x[5])),
            float(x[6]),
            float(x[7]),
            float(x[8]),
            float(x[9]),
            (float(x[10])),
        ),
    )
).map(lambda x: LabeledPoint(x[0], x[1]))
print(GM_RDD__labeledPoint.first())


(trainingData, testData) = GM_RDD__labeledPoint.randomSplit([0.7, 0.3])

# Train a RandomForest model.
dictRF = {}
for i in (3, 5, 8, 10, 12, 14, 16, 18, 20, 22, 24):
    model = RandomForest.trainRegressor(
        trainingData,
        categoricalFeaturesInfo={},
        numTrees=i,
        featureSubsetStrategy="auto",
        impurity="variance",
        maxDepth=4,
        maxBins=32,
    )

    # Evaluate model on test instances and compute test error
    predictions = model.predict(testData.map(lambda x: x.features))
    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
    testMSE = labelsAndPredictions.map(
        lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])
    ).sum() / float(testData.count())
    dictRF[i] = testMSE
    best_key = min(dictRF, key=dictRF.get)
print(best_key)
print(dictRF)

# Train the model again in the best number of Trees
model_rf_optimal = RandomForest.trainRegressor(
    trainingData,
    categoricalFeaturesInfo={},
    numTrees=12,
    featureSubsetStrategy="auto",
    impurity="variance",
    maxDepth=4,
    maxBins=32,
)
print("Test Mean Squared Error = " + str(testMSE))

# Save and load model and test
model_rf_optimal.save(sc, "../BPMP2_RandomForestRegressionModel")
model_rf_optimal_reuse = RandomForestModel.load(sc, "../BPMP2_kmeans")
rdd_predict = sc.parallelize(
    (
        0.0,
        [
            0.0,
            2.0,
            0.708899974822998,
            0.11100000143051147,
            0.5529999732971191,
            0.335999995470047,
            5.0,
        ],
    )
)
print(model_rf_optimal_reuse.predict(rdd_predict).collect)
