from sre_constants import RANGE
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from pyspark import SparkConf
from pyspark.context import SparkContext
from numpy import array
from math import sqrt
from pyspark.mllib.clustering import KMeans, KMeansModel
from pyspark.mllib.feature import StandardScaler

if __name__ == "__main__":
    spark = (
        SparkSession.builder.master(f"local[*]")
        .appName("myApp")
        .config(
            "spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1"
        )
        .getOrCreate()
    )

    # Pull the data from mongo into RDDs
    GM_DF_isResponse = (
        spark.read.format("mongo")
        .option("uri", f"mongodb://127.0.0.1/bdm_project2022.GM_DF_isResponse")
        .load()
    )
    GM_DF__sentenceSentimentScore = (
        spark.read.format("mongo")
        .option(
            "uri", f"mongodb://127.0.0.1/bdm_project2022.GM_DF__sentenceSentimentScore"
        )
        .load()
    )
GM_RDD_isResponse = GM_DF_isResponse.rdd
print(GM_RDD_isResponse.first())
GM_RDD__sentenceSentimentScore = GM_DF__sentenceSentimentScore.rdd
print(GM_RDD__sentenceSentimentScore.first())

# Get mapping from restaurantID to restaurantName
GM_RDD__restaurantKeysNames = GM_RDD__sentenceSentimentScore.map(
    lambda x: (x[3], x[4])
).distinct()
print(GM_RDD__restaurantKeysNames.first())
# Top 10 restaurant with highest responde rate
def isResponse(x):
    if x == None:
        a = 0
    else:
        a = 1
    return a


GM_RDD__respondeRate = (
    GM_RDD_isResponse.map(lambda x: (x[2], (isResponse(x[1]), 1)))
    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
    .mapValues(lambda x: x[0] / x[1])
)
print(GM_RDD__respondeRate.first())

print("The top 10 restaurants with highest respond rate is:")
sc = SparkContext.getOrCreate()
print(
    sc.parallelize(
        GM_RDD__respondeRate.sortBy(lambda x: x[1], ascending=False).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest average Stars
print("# Top 10 restaurant with highest average Stars")
GM_RDD__averageStar = (
    GM_RDD__sentenceSentimentScore.map(lambda x: (x[3], (int(float(x[10])), 1)))
    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
    .mapValues(lambda x: x[0] / x[1])
)
print(
    sc.parallelize(GM_RDD__averageStar.sortBy(lambda x: x[1], ascending=False).take(10))
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest average Compound Sentiment scores
print("Top 10 restaurant with highest average Compound Sentiment scores")
GM_RDD__averageSentiment = (
    GM_RDD__sentenceSentimentScore.map(lambda x: (x[3], (float(x[6]), 1)))
    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
    .mapValues(lambda x: x[0] / x[1])
)
print(
    sc.parallelize(
        GM_RDD__averageSentiment.sortBy(lambda x: x[1], ascending=False).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest average Compound Sentiment scores Adjusted by Likes
print(
    "Top 10 restaurant with highest average Compound Sentiment scores Adjusted by Likes"
)
GM_RDD__averageSentimentAdjustedByLikes = (
    GM_RDD__sentenceSentimentScore.map(
        lambda x: (
            x[3],
            ((float(x[6]) * (int(float(x[2])) + 1)), (int(float(x[2])) + 1)),
        )
    )
    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
    .mapValues(lambda x: x[0] / x[1])
)

print(
    sc.parallelize(
        GM_RDD__averageSentimentAdjustedByLikes.sortBy(
            lambda x: x[1], ascending=False
        ).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest sum of reviews
print("# Top 10 restaurant with highest sum of reviews")
GM_RDD__sumNumberReviews = GM_RDD__sentenceSentimentScore.map(
    lambda x: (x[3], 1)
).reduceByKey(lambda x, y: x + y)
print(
    sc.parallelize(
        GM_RDD__sumNumberReviews.sortBy(lambda x: x[1], ascending=False).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest sum of likes
print("# Top 10 restaurant with highest sum of likes")
GM_RDD__sumNumberLikes = GM_RDD__sentenceSentimentScore.map(
    lambda x: (x[3], int(float(x[2])))
).reduceByKey(lambda x, y: x + y)
print(
    sc.parallelize(
        GM_RDD__sumNumberLikes.sortBy(lambda x: x[1], ascending=False).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest averageReviewerNumberOfReviews
print("# Top 10 restaurant with highest averageReviewerNumberOfReviews")
GM_RDD__averageReviewerNumberOfReviews = (
    GM_RDD__sentenceSentimentScore.map(lambda x: (x[3], (int(float(x[5])), 1)))
    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
    .mapValues(lambda x: x[0] / x[1])
)
print(
    sc.parallelize(
        GM_RDD__averageReviewerNumberOfReviews.sortBy(
            lambda x: x[1], ascending=False
        ).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# Top 10 restaurant with highest amount of low-score reviews
print("# Top 10 restaurant with highest amount of low-score reviews")
GM_RDD__sumNumberLowScoreReview = (
    GM_RDD__sentenceSentimentScore.filter(lambda x: int(float(x[10])) < 3)
    .map(lambda x: (x[3], 1))
    .reduceByKey(lambda x, y: x + y)
)
GM_RDD__sumNumberLowScoreReviewRatio = GM_RDD__sumNumberLowScoreReview.join(
    GM_RDD__sumNumberReviews
).mapValues(lambda x: x[0] / x[1])
print(
    sc.parallelize(
        GM_RDD__sumNumberLowScoreReviewRatio.sortBy(
            lambda x: x[1], ascending=False
        ).take(10)
    )
    .join(GM_RDD__restaurantKeysNames)
    .map(lambda x: (x[0], x[1][1], x[1][0]))
    .collect()
)

# join the the above aggregated new features together
print("# join the the above aggregated new features together")
GM_RDD__restaurant = (
    GM_RDD__sumNumberLowScoreReviewRatio.join(GM_RDD__averageReviewerNumberOfReviews)
    .join(GM_RDD__sumNumberReviews)
    .join(GM_RDD__averageStar)
    .join(GM_RDD__respondeRate)
    .join(GM_RDD__averageSentiment)
    .join(GM_RDD__averageSentimentAdjustedByLikes)
    .join(GM_RDD__sumNumberLikes)
    .map(
        lambda x: (
            x[0],
            (
                x[1][0][0][0][0][0][0][0],
                x[1][0][0][0][0][0][0][1],
                x[1][0][0][0][0][0][1],
                x[1][0][0][0][0][1],
                x[1][0][0][0][1],
                x[1][0][0][1],
                x[1][0][1],
                x[1][1],
            ),
        )
    )
)
print(GM_RDD__restaurant.first())

# Scaling the input values of KNN algorithm
dictR = {}
for i in range(0, 8):
    dictR[i] = (
        GM_RDD__restaurant.map(lambda x: x[1][i]).min(),
        GM_RDD__restaurant.map(lambda x: x[1][i]).max(),
    )
GM_RDD__restaurant_minMaxScaler = GM_RDD__restaurant.mapValues(
    lambda x: (
        (x[0:8]),
        (
            (x[0] - dictR[0][0]) / dictR[0][1],
            (x[1] - dictR[1][0]) / dictR[1][1],
            (x[2] - dictR[2][0]) / dictR[2][1],
            (x[3] - dictR[3][0]) / dictR[3][1],
            (x[4] - dictR[4][0]) / dictR[4][1],
            (x[5] - dictR[5][0]) / dictR[5][1],
            (x[6] - dictR[6][0]) / dictR[6][1],
            (x[7] - dictR[7][0]) / dictR[7][1],
        ),
    )
).join(GM_RDD__restaurantKeysNames)
print(GM_RDD__restaurant_minMaxScaler.first())


# Take only vector data
GM_RDD__restaurant_minMaxScaler_vector = GM_RDD__restaurant_minMaxScaler.map(
    lambda x: x[1][0][1]
)
# Build the intial KNN model with 20 clusters
clusters = KMeans.train(
    GM_RDD__restaurant_minMaxScaler_vector,
    20,
    maxIterations=10,
    initializationMode="random",
)

# Evaluate clustering by computing Within Set Sum of Squared Errors
def error(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x ** 2 for x in (point - center)]))


WSSSE = GM_RDD__restaurant_minMaxScaler_vector.map(lambda point: error(point)).reduce(
    lambda x, y: x + y
)
print(
    "If we devide the restaurants into 20 clusters, Within Set Sum of Squared Error = "
    + str(WSSSE)
)

# Show all members of each cluster
dictK = {}
for i in range(0, len(clusters.centers)):
    dictK[i] = clusters.centers[i]


def belongClusterK(x):
    for i in range(0, 20):
        if clusters.predict(x) == clusters.predict(dictK[i]):
            return i


GM_RDD__restaurant_cluster = GM_RDD__restaurant_minMaxScaler.map(
    lambda x: (x[0], x[1], belongClusterK(x[1][0][1]))
)
GM_RDD__restaurant_cluster_aggregate = GM_RDD__restaurant_cluster.map(
    lambda x: (x[2], x[1][1])
).reduceByKey(lambda x, y: x + ";   " + y)
print("If we devide the restaurants into 20 clusters, each cluster's members are: ")
print(GM_RDD__restaurant_cluster_aggregate.collect())

# Find the optimal K hyperparameter
print(
    "Since we have 178 restaurants, now iterate possible K hyperparameter values [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]"
)
dictKElbow = {}
for i in [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]:
    clusters = KMeans.train(
        GM_RDD__restaurant_minMaxScaler_vector,
        i,
        maxIterations=10,
        initializationMode="random",
    )
    WSSSE = GM_RDD__restaurant_minMaxScaler_vector.map(
        lambda point: error(point)
    ).reduce(lambda x, y: x + y)
    dictKElbow[i] = str(WSSSE)
import matplotlib.pyplot as plt

KValues = list(dictKElbow.keys())
WSSSE = list(dictKElbow.values())
ax = plt.gca()
ax.invert_yaxis()
plt.plot(KValues, WSSSE, "b")
plt.title("The WSSSE with KValues")
plt.xlabel("KValues")
plt.ylabel("WSSSE")
plt.legend()

plt.show()

# Save and load model
print("Now save the best K(=10) model into MongoDB")
clusters_optimal = KMeans.train(
    GM_RDD__restaurant_minMaxScaler_vector,
    10,
    maxIterations=10,
    initializationMode="random",
)


clusters_optimal.save(sc, "../BPMP2_kmeans")
clusters_optimal_reuse = KMeansModel.load(sc, "../BPMP2_kmeans")


# predict if two restaurants belong to same cluster
print(
    clusters_optimal_reuse.predict(
        (0.0, 0.0, 0.0, 0.006329113924050801, 0.0, 0.0, 0.0, 0.8125)
    )
    == clusters_optimal_reuse.predict(
        (0.0, 0.0, 0.0, 0.006329113924050801, 0.0, 0.0, 0.0, 0.8125)
    )
)

